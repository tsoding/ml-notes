\documentclass{article}

\usepackage{amsmath}
\usepackage{tikz}

\begin{document}
\section{Gradient Descent}

If we keep decreasing the $\epsilon$ in our Finite Difference approach we effectively get the Derivative of the Cost Function.

\begin{align}
  C'(w) = \lim_{\epsilon \to 0}\frac{C(w + \epsilon) - C(w)}{\epsilon}
\end{align}

Let's compute the derivatives of all our models. Throughout the entire paper $n$ means the amount of samples in the training set.

\subsection{Linear Model}

\def\d{2.0}

\begin{center}
  \begin{tikzpicture}
    \node (X) at ({-\d*0.75}, 0) {$x$};
    \node[shape=circle,draw=black] (N) at (0, 0) {$w$};
    \node (Y) at ({\d*0.75}, 0) {$y$};
    \path[->] (X) edge (N);
    \path[->] (N) edge (Y);
  \end{tikzpicture}
\end{center}

\begin{align}
  y &= x \cdot w
\end{align}

\subsubsection{Cost}

\begin{align}
  C(w) &= \frac{1}{n}\sum_{i=1}^{n}(x_iw - y_i)^2 \\
  C'(w)
       &= \left(\frac{1}{n}\sum_{i=1}^{n}(x_iw - y_i)^2\right)' = \\
       &= \frac{1}{n}\left(\sum_{i=1}^{n}(x_iw - y_i)^2\right)' \\
       &= \frac{1}{n}\sum_{i=1}^{n}\left((x_iw - y_i)^2\right)' \\
       &= \frac{1}{n}\sum_{i=1}^{n}2(x_iw - y_i)x_i
\end{align}

\subsection{One Neuron Model with 2 inputs}

\begin{center}
  \begin{tikzpicture}
    \node (X) at (-\d, 1) {$x$};
    \node (Y) at (-\d, -1) {$y$};
    \node[shape=circle,draw=black] (N) at (0, 0) {$\sigma, b$};
    \node (Z) at (\d, 0) {$z$};
    \path[->] (X) edge node[above] {$w_1$} (N);
    \path[->] (Y) edge node[above] {$w_2$} (N);
    \path[->] (N) edge (Z);
  \end{tikzpicture}
\end{center}
\begin{align}
  z &= \sigma(xw_1 + yw_2 + b) \\
  \sigma(x) &= \frac{1}{1 + e^{-x}} \\
  \sigma'(x) &= \sigma(x)(1 - \sigma(x))
\end{align}

\subsubsection{Cost}

\def\pd[#1]{\partial_{#1}}
\def\avgsum[#1,#2]{\frac{1}{#2}\sum_{#1=1}^{#2}}
\begin{align}
  a_i &= \sigma(x_iw_1 + y_iw_2 + b) \\
  \pd[w_1]a_i
      &= \pd[w_1](\sigma(x_iw_1 + y_iw_2 + b)) = \\
      &= a_i(1 - a_i)\pd[w_1](x_iw_1 + y_iw_2 + b) = \\
      &= a_i(1 - a_i)x_i \\
  \pd[w_2]a_i &= a_i(1 - a_i)y_i \\
  \pd[b]a_i &= a_i(1 - a_i) \\
  C &= \avgsum[i, n](a_i - z_i)^2 \\
  \pd[w_1] C
      &= \avgsum[i, n]\pd[w_1]\left((a_i - z_i)^2\right) = \\
      &= \avgsum[i, n]2(a_i - z_i)\pd[w_1]a_i = \\
      &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)x_i \\
  \pd[w_2] C &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)y_i \\
  \pd[b] C &= \avgsum[i, n]2(a_i - z_i)a_i(1 - a_i)
\end{align}

\subsection{Two Neurons Model with 1 input}

\begin{center}
  \begin{tikzpicture}
    \node (X) at (-\d, 0) {$x$};
    \node[shape=circle,draw=black] (N1) at (0, 0) {$\sigma, b^{(1)}$};
    \node[shape=circle,draw=black] (N2) at (\d, 0) {$\sigma, b^{(2)}$};
    \node (Y) at ({2*\d}, 0) {$y$};
    \path[->] (X) edge node[above] {$w^{(1)}$} (N1);
    \path[->] (N1) edge node[above] {$w^{(2)}$} (N2);
    \path[->] (N2) edge (Y);
  \end{tikzpicture}
\end{center}

\begin{align}
  a^{(1)} &= \sigma(xw^{(1)} + b^{(1)}) \\
  y &= \sigma(a^{(1)}w^{(2)} + b^{(2)})
\end{align}

The superscript in parenthesis denotes the current layer. For example $a_i^{(l)}$ denotes the activation from the $l$-th layer on $i$-th sample.

\subsubsection{Feed-Forward}

\begin{align}
  a_i^{(1)} &= \sigma(x_iw^{(1)} + b^{(1)}) \\
  \pd[w^{(1)}]a_i^{(1)} &= a_i^{(1)}(1 - a_i^{(1)})x_i \\
  \pd[b^{1}]a_i^{(1)} &= a_i^{(1)}(1 - a_i^{(1)}) \\
  a_i^{(2)} &= \sigma(a_i^{(1)}w^{(2)} + b^{(2)}) \\
  \pd[w^{(2)}]a_i^{(2)} &= a_i^{(2)}(1 - a_i^{(2)})a_i^{(1)} \\
  \pd[b^{(2)}]a_i^{(2)} &= a_i^{(2)}(1 - a_i^{(2)}) \\
  \pd[a_i^{(1)}]a_i^{(2)} &= a_i^{(2)}(1 - a_i^{(2)})w^{(2)}
\end{align}

\subsubsection{Back-Propagation}

We observe that $a_i^{(2)}$ is dependent only on $a_i^{(1)}$ and not on any of the other $a_j^{(2)}$'s ($i \neq j$), since they represent different samples.

\begin{align}
  C &= \avgsum[i, n] (a_i^{(2)} - y_i)^2 \\
  \pd[a_i^{(2)}] C
          &= \pd[a_i^{(2)}] \left( \avgsum[j, n] (a_j^{(2)} - y_j)^2 \right) = \\
          &= \frac{1}{n} \pd[a_i^{(2)}] ((a_i^{(2)} - y_i)^2) = \\
          &= \frac{1}{n} \cdot 2(a_i^{(2)} - y_i)\\
  \pd[a_i^{(1)}] C
          &= \pd[a_i^{(2)}] C \cdot \pd[a_i^{(1)}] a_i^{(2)} = \\
          &= \frac{1}{n} \cdot 2(a_i^{(2)} - y_i) a_i^{(2)}(1 - a_i^{(2)})w^{(2)}
\end{align}

To evaluate the desired gradients, the chain rule for partial differentiation must be repeatedly applied to peel off the surrounding layers. Repetition can be minimized by re-using the results from the above calculation and from the previous section.

\begin{align}
  \pd[w^{(2)}] C
          &= \avgsum[i, n] \pd[a_i^{(2)}] C \cdot \pd[w^{(2)}] a_i^{(2)} = \\
          &= \avgsum[i, n] 2(a_i^{(2)} - y_i)a_i^{(2)}(1 - a_i^{(2)})a_i^{(1)}\\
  \pd[b^{(2)}] C
          &= \avgsum[i, n] \pd[a_i^{(2)}] C \cdot \pd[b^{(2)}] a_i^{(2)} = \\
          &= \avgsum[i, n] 2(a_i^{(2)} - y_i)a_i^{(2)}(1 - a_i^{(2)})\\
  \pd[w^{(1)}] C
            &= \sum_{i=1}^n \pd[a_i^{(1)}] C \cdot \pd[w^{(1)}] a_i^{(1)} = \\
            &= \sum_{i=1}^n \pd[a_i^{(1)}] C \cdot a_i^{(1)}(1 - a_i^{(1)})x_i \\
  \pd[b^{(1)}] C
            &= \sum_{i=1}^n \pd[a_i^{(1)}] C \cdot \pd[b^{(1)}] a_i^{(1)} = \\
            &= \sum_{i=1}^n \pd[a_i^{(1)}] C \cdot a_i^{(1)}(1 - a_i^{(1)})
\end{align}

As a visual aid, the following diagram depicts the dependencies between variables.

\begin{center}
  \begin{tikzpicture}
    \node (X) at (-{1.5*\d}, 0) {$x$};
    \node[shape=circle,draw=black] (A1) at (0, 0) {$a_i^{(1)}$};
    \node (W1) at (0, \d) {$w^{(1)}$};
    \node (B1) at (0, -\d) {$b^{(1)}$};
    \node[shape=circle,draw=black] (A2) at ({1.5*\d}, 0) {$a_i^{(2)}$};
    \node (W2) at ({1.5*\d}, \d) {$w^{(2)}$};
    \node (B2) at ({1.5*\d}, -\d) {$b^{(2)}$};
    \node (C) at ({3*\d}, 0) {$C$};
    \node (Y) at ({3*\d}, -\d) {$y$};
    \path[->] (X) edge (A1);
    \path[->] (W1) edge node[left] {$\pd[w^{(1)}] a_i^{(1)}$} (A1);
    \path[->] (B1) edge node[left] {$\pd[b^{(1)}] a_i^{(1)}$} (A1);
    \path[->] (A1) edge node[above] {$\pd[a_i^{(1)}] a_i^{(2)}$} (A2);
    \path[->] (W2) edge node[right] {$\pd[w^{(2)}] a_i^{(2)}$} (A2);
    \path[->] (B2) edge node[right] {$\pd[b^{(2)}] a_i^{(2)}$} (A2);
    \path[->] (A2) edge node[above] {$\pd[a_i^{(2)}] C$} (C);
    \path[->] (Y) edge (C);
  \end{tikzpicture}
\end{center}

Differentiating $C$ by a variable amounts to multiplying all the partial differential coefficients encountered along the path from that variable to $C$. Paths through different $i$'s can be thought of as parallel, products along which are to be summed at the end.

\subsection{Arbitrary Neurons Model with 1 input}

Let's assume that we have $m$ layers.

\subsubsection{Feed-Forward}

Let's assume that $a_i^{(0)}$ is $x_i$.

\begin{align}
  a_i^{(l)} &= \sigma(a_i^{(l-1)}w^{(l)} + b^{(l)}) \\
  \pd[w^{(l)}]a_i^{(l)} &= a_i^{(l)}(1 - a_i^{(l)})a_i^{(l-1)} \\
  \pd[b^{(l)}]a_i^{(l)} &= a_i^{(l)}(1 - a_i^{(l)}) \\
  \pd[a_i^{(l-1)}]a_i^{(l)} &= a_i^{(l)}(1 - a_i^{(l)})w^{(l)}
\end{align}

\subsubsection{Back-Propagation: a conventional approach}

\begin{align}
  C &= \avgsum[i, n] (a_i^m - y_i)^2 \\
  \pd[a_i^{(m)}] C &= \frac{1}{n} \cdot 2(a_i^{(m)} - y_i) \\
  \pd[w^{(l)}] C
        &= \sum_{i=1}^n \pd[a_i^{(l)}] C \cdot \pd[w^{(l)}] a_i^{(l)} = \\
        &= \sum_{i=1}^n \pd[a_i^{(l)}] C \cdot a_i^{(l)}(1 - a_i^{(l)})a_i^{(l-1)} \\
  \pd[b^{(l)}] C
        &= \sum_{i=1}^n \pd[a_i^{(l)}] C \cdot \pd[b^{(l)}] a_i^{(l)} = \\
        &= \sum_{i=1}^n \pd[a_i^{(l)}] C \cdot a_i^{(l)}(1 - a_i^{(l)}) \\
  \pd[a_i^{(l-1)}] C
        &= \pd[a_i^{(l)}] C \cdot \pd[a_i^{(1-1)}] a_i^{(l)} \\
        &= \pd[a_i^{(l)}] C \cdot a_i^{(l)}(1 - a_i^{(l)})w^{(l)}
\end{align}

\end{document}
